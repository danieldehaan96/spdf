


#R script for Master Thesis Daniel de Haan

###   CODE   ###

####          Libraries used. ####
library(xlsx)
library(tsintermittent)
library(lightgbm)
library(readxl)
library(boot)
library(expm)
library(igraph)
library(matlab)
library(markovchain)
library(Rcpp)
library(TraMineR)
library(dplyr)
library(RSNNS)
library(tidyr)
library(collapse)
library(runner)
library(ggplot2)
library(data.table)
library(greybox)
library(beepr)

# Set seed for reproducibility.
set.seed(8377)

# Simulating the four simulated data sets.
# SIM1 <- simID(n=6500, obs=60, idi=1, cv2=0.75, level=10)
# SIM2 <- simID(n=6500, obs=60, idi=1.5, cv2=0.8, level=10)
# SIM3 <- simID(n=6500, obs=60, idi=1.05, cv2=0.3, level=10)
# SIM4 <- simID(n=6500, obs=60, idi=1.45, cv2=0.25, level=10)

# Saving them as Excel files for simple calculations.
# write.xlsx(SIM1, "c:/Daan/SIM1.xlsx")
# write.xlsx(SIM2, "c:/Daan/SIM2.xlsx")
# write.xlsx(SIM3, "c:/Daan/SIM3.xlsx")
# write.xlsx(SIM4, "c:/Daan/SIM42.xlsx")

####          Importing the simulated and industrial data sets. ####
SIM1 <- as.data.frame(read_excel("C:/Daan/SIM1.xlsx"))
SIM2 <- as.data.frame(read_excel("C:/Daan/SIM2.xlsx"))
SIM3 <- as.data.frame(read_excel("C:/Daan/SIM3.xlsx"))
SIM4 <- as.data.frame(read_excel("C:/Daan/SIM4.xlsx"))

MAN <- as.data.frame(read_excel("C:/Daan/MAN.xlsx", sheet = "GOEIE"))
BRAF <- as.data.frame(t(read_excel("C:/Daan/BRAF.xls", sheet = "GOEIE")))[-c(1), ]
AUTO <- as.data.frame(t(read_excel("C:/Daan/AUTO.xls", sheet = "GOEIE")))[-c(1), ]
OIL <- as.data.frame(t(read_excel("C:/Daan/OIL.xls", sheet = "GOEIE")))[-c(1), ]

# Extracting price and leadtime vectors.
pricesSIM4 <- SIM4[61,]
leadtimesSIM4 <- 1
SIM4 <- SIM4[-61,]

# Coercing all negative values to zeroes.

MAN <- pmax(MAN,0)
BRAF <- pmax(BRAF,0)
AUTO <- pmax(AUTO,0)
OIL <- pmax(OIL,0)

####          Creating the test and train data sets. ####
# As the sample size is the same for all simulated data sets, the sample from SIM1 can just be used for all SIM data sets.
sampleSIM1 = round(nrow(SIM1)*.70) 
trainSIM1 <- SIM1[1:(sampleSIM1), ]
testSIM1 <- SIM1[-(1:(sampleSIM1)), ]

trainSIM2 <- SIM2[1:(sampleSIM1), ]
testSIM2 <- SIM2[-(1:(sampleSIM1)), ]

trainSIM3 <- SIM3[1:(sampleSIM1), ]
testSIM3 <- SIM3[-(1:(sampleSIM1)), ]

trainSIM4 <- SIM4[1:(sampleSIM1), ]
testSIM4 <- SIM4[-(1:(sampleSIM1)), ]

# The split point is established for the industrial data sets individually.
sampleMAN = round(nrow(MAN)*.70)
trainMAN <- MAN[1:(sampleMAN), ]
testMAN <- MAN[-(1:(sampleMAN)), ]

sampleBRAF = round(nrow(BRAF)*.70)
trainBRAF <- BRAF[1:(sampleBRAF), ]
testBRAF <- BRAF[-(1:(sampleBRAF)), ]

sampleAUTO = round(nrow(AUTO)*.70)
trainAUTO <- AUTO[1:(sampleAUTO), ]
testAUTO <- AUTO[-(1:(sampleAUTO)), ]

sampleOIL = round(nrow(OIL)*.70)
trainOIL <- OIL[1:(sampleOIL), ]
testOIL <- OIL[-(1:(sampleOIL)), ]

# Dropping the items which do not have more than one demand occurence for the industrial data sets.


cond1 <- colSums(trainMAN != 0, na.rm=T)<2
mask1 <- !(cond1)
trainMAN <- subset(trainMAN, select=mask1)
testMAN <- subset(testMAN,select=names(trainMAN))


cond2 <- colSums(trainBRAF != 0, na.rm=T)<2
mask2 <- !(cond2)
trainBRAF <- subset(trainBRAF, select=mask2)
testBRAF <- subset(testBRAF,select=names(trainBRAF))

cond3 <- colSums(trainAUTO != 0, na.rm=T)<2
mask3 <- !(cond3)
trainAUTO <- subset(trainAUTO, select=mask3)
testAUTO <- subset(testAUTO,select=names(trainAUTO))

cond4 <- colSums(trainOIL != 0, na.rm=T)<2
mask4 <- !(cond4)
trainOIL <- subset(trainOIL, select=mask4)
testOIL <- subset(testOIL,select=names(trainOIL))

# The forecast horizon is set to the amount of periods in the test sample.

####          Croston on all data sets. ####
ptm <- proc.time()

####          Croston on SIM1 data ####
CrostonSIM1 <- data.frc(trainSIM1, method="crost",h=18, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSESIM1 <- NULL
CrostonMASESIM1 <- NULL
CrostonRMSSESIM1 <- NULL
for (i in 1:ncol(testSIM1)){
  CrostonMSESIM1 <- cbind(CrostonMSESIM1,  MSE(t(testSIM1[i]),t(CrostonSIM1[i])))
  CrostonMASESIM1 <- cbind(CrostonMASESIM1,  MASE(t(testSIM1[i]),t(CrostonSIM1[i]), mean(abs(t(trainSIM1[i])))))
  CrostonRMSSESIM1 <- cbind(CrostonRMSSESIM1,  RMSSE(t(testSIM1[i]),t(CrostonSIM1[i]), mean(abs(t(trainSIM1[i])))))
}
CrostonMSESIM1 <- mean(CrostonMSESIM1)
CrostonMASESIM1 <- mean(CrostonMASESIM1)
CrostonRMSSESIM1 <- mean(CrostonRMSSESIM1)
print(c("SIM1", CrostonMSESIM1, CrostonMASESIM1, CrostonRMSSESIM1))



####          Croston on SIM2 data ####
CrostonSIM2 <- data.frc(trainSIM2, method="crost", h=18, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSESIM2 <- NULL
CrostonMASESIM2 <- NULL
CrostonRMSSESIM2 <- NULL
for (i in 1:ncol(testSIM2)){
  CrostonMSESIM2 <- cbind(CrostonMSESIM2,  MSE(t(testSIM2[i]),t(CrostonSIM2[i])))
  CrostonMASESIM2 <- cbind(CrostonMASESIM2,  MASE(t(testSIM2[i]),t(CrostonSIM2[i]), mean(abs(t(trainSIM2[i])))))
  CrostonRMSSESIM2 <- cbind(CrostonRMSSESIM2,  RMSSE(t(testSIM2[i]),t(CrostonSIM2[i]), mean(abs(t(trainSIM2[i])))))
}
CrostonMSESIM2 <- mean(CrostonMSESIM2)
CrostonMASESIM2 <- mean(CrostonMASESIM2)
CrostonRMSSESIM2 <- mean(CrostonRMSSESIM2)
print(c("SIM2", CrostonMSESIM2, CrostonMASESIM2, CrostonRMSSESIM2))


####          Croston on SIM3 data ####
CrostonSIM3 <- data.frc(trainSIM3, method="crost", h=18, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSESIM3 <- NULL
CrostonMASESIM3 <- NULL
CrostonRMSSESIM3 <- NULL
for (i in 1:ncol(testSIM3)){
  CrostonMSESIM3 <- cbind(CrostonMSESIM3,  MSE(t(testSIM3[i]),t(CrostonSIM3[i])))
  CrostonMASESIM3 <- cbind(CrostonMASESIM3,  MASE(t(testSIM3[i]),t(CrostonSIM3[i]), mean(abs(t(trainSIM3[i])))))
  CrostonRMSSESIM3 <- cbind(CrostonRMSSESIM3,  RMSSE(t(testSIM3[i]),t(CrostonSIM3[i]), mean(abs(t(trainSIM3[i])))))
}
CrostonMSESIM3 <- mean(CrostonMSESIM3)
CrostonMASESIM3 <- mean(CrostonMASESIM3)
CrostonRMSSESIM3 <- mean(CrostonRMSSESIM3)
print(c("SIM3", CrostonMSESIM3, CrostonMASESIM3, CrostonRMSSESIM3))


####          Croston on SIM4 data ####
CrostonSIM4 <- data.frc(trainSIM4, method="crost", h=18, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSESIM4 <- NULL
CrostonMASESIM4 <- NULL
CrostonRMSSESIM4 <- NULL
for (i in 1:ncol(testSIM4)){
CrostonMSESIM4 <- cbind(CrostonMSESIM4,  MSE(t(testSIM4[i]),t(CrostonSIM4[i])))
CrostonMASESIM4 <- cbind(CrostonMASESIM4,  MASE(t(testSIM4[i]),t(CrostonSIM4[i]), mean(abs(t(trainSIM4[i])))))
CrostonRMSSESIM4 <- cbind(CrostonRMSSESIM4,  RMSSE(t(testSIM4[i]),t(CrostonSIM4[i]), mean(abs(t(trainSIM4[i])))))
}
CrostonMSESIM4 <- mean(CrostonMSESIM4)
CrostonMASESIM4 <- mean(CrostonMASESIM4)
CrostonRMSSESIM4 <- mean(CrostonRMSSESIM4)
print(c("SIM4", CrostonMSESIM4, CrostonMASESIM4, CrostonRMSSESIM4))

# Inventory performance measures.
targetfillrates <- c(qnorm(0.75),qnorm(0.8),qnorm(0.85),qnorm(0.9),qnorm(0.95),qnorm(0.99),qnorm(0.999999))
leadtimes <- as.data.frame(leadtimesSIM4)
prices <- as.data.frame(pricesSIM4)

i=3

holdingcostsSIM4 <- matrix(c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999),ncol = 7)
for (i in 1:ncol(CrostonSIM4)){
leadtimedemand <- mean(t(CrostonSIM4[i])) * 1 # * leadtimes[,i]
stdev <- std(t(trainSIM4[i]))
stocklevelR <- (targetfillrates * stdev + leadtimedemand)
holdingcostsSIM4 <- rbind(holdingcostsSIM4,(0.25 * (stocklevelR * prices[,i])))
}
CrostonholdingSIM4 <- colSums(holdingcostsSIM4[-1,])
ProcentualCrostonholdingSIM4 <- CrostonholdingSIM4/CrostonholdingSIM4[1]

CrostonholdingSIM4 <- holdingcostsSIM4[-1,]

# Achieved fill rate.
fillrateCrostonSIM4 <- matrix(ncol=7)
averagefillrateitem <- matrix(ncol=7)
fillratefiller <- matrix(rep(NA,18),ncol=nrow(testSIM4))
for(i in 1:ncol(testSIM4)){
  for (j in 1:length(targetfillrates)){
  fillratefiller <- stocklevelR[j] / t(testSIM4[i])[(t(testSIM4[i]) > 0)]
  fillratefiller[fillratefiller >= 1] <- 1
  averagefillrateitem[j] <- mean(fillratefiller)
  }
  fillrateCrostonSIM4 <- rbind(fillrateCrostonSIM4, averagefillrateitem)
  }
achievedfillrateCrostonSIM4 <- colMeans(fillrateCrostonSIM4[-1,])

achievedfillrateCrostonSIM4 <- fillrateCrostonSIM4[-1,]

ServicelevelCrostonSIM4 <- data.frame(achievedfillrateCrostonSIM4 = achievedfillrateCrostonSIM4, CrostonholdingSIM4 = ProcentualCrostonholdingSIM4, targetfillrates = c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999))

####          Croston on MAN data ####
CrostonMAN <- data.frc(trainMAN, method="crost", h=45, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSEMAN <- NULL
CrostonMASEMAN <- NULL
CrostonRMSSEMAN <- NULL
for (i in 1:ncol(testMAN)){
  CrostonMSEMAN <- cbind(CrostonMSEMAN,  MSE(t(testMAN[i]),t(CrostonMAN[i])))
  CrostonMASEMAN <- cbind(CrostonMASEMAN,  MASE(t(testMAN[i]),t(CrostonMAN[i]), mean(abs(t(trainMAN[i])))))
  CrostonRMSSEMAN <- cbind(CrostonRMSSEMAN,  RMSSE(t(testMAN[i]),t(CrostonMAN[i]), mean(abs(t(trainMAN[i])))))
}
CrostonMSEMAN <- mean(CrostonMSEMAN)
CrostonMASEMAN <- mean(CrostonMASEMAN)
CrostonRMSSEMAN <- mean(CrostonRMSSEMAN)
print(c("MAN", CrostonMSEMAN, CrostonMASEMAN, CrostonRMSSEMAN))

####          Croston on BRAF data ####
CrostonBRAF <- data.frc(trainBRAF, method="crost", h=25, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSEBRAF <- NULL
CrostonMASEBRAF <- NULL
CrostonRMSSEBRAF <- NULL
for (i in 1:ncol(testBRAF)){
  CrostonMSEBRAF <- cbind(CrostonMSEBRAF,  MSE(t(testBRAF[i]),t(CrostonBRAF[i])))
  CrostonMASEBRAF <- cbind(CrostonMASEBRAF,  MASE(t(testBRAF[i]),t(CrostonBRAF[i]), mean(abs(t(trainBRAF[i])))))
  CrostonRMSSEBRAF <- cbind(CrostonRMSSEBRAF,  RMSSE(t(testBRAF[i]),t(CrostonBRAF[i]), mean(abs(t(trainBRAF[i])))))
}
CrostonMSEBRAF <- mean(CrostonMSEBRAF)
CrostonMASEBRAF <- mean(CrostonMASEBRAF)
CrostonRMSSEBRAF <- mean(CrostonRMSSEBRAF)
print(c("BRAF", CrostonMSEBRAF, CrostonMASEBRAF, CrostonRMSSEBRAF))

####          Croston on AUTO data ####
CrostonAUTO <- data.frc(trainAUTO, method="crost", h=7, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSEAUTO <- NULL
CrostonMASEAUTO <- NULL
CrostonRMSSEAUTO <- NULL
for (i in 1:ncol(testAUTO)){
  CrostonMSEAUTO <- cbind(CrostonMSEAUTO,  MSE(t(testAUTO[i]),t(CrostonAUTO[i])))
  CrostonMASEAUTO <- cbind(CrostonMASEAUTO,  MASE(t(testAUTO[i]),t(CrostonAUTO[i]), mean(abs(t(trainAUTO[i])))))
  CrostonRMSSEAUTO <- cbind(CrostonRMSSEAUTO,  RMSSE(t(testAUTO[i]),t(CrostonAUTO[i]), mean(abs(t(trainAUTO[i])))))
}
CrostonMSEAUTO <- mean(CrostonMSEAUTO)
CrostonMASEAUTO <- mean(CrostonMASEAUTO)
CrostonRMSSEAUTO <- mean(CrostonRMSSEAUTO)
print(c("AUTO", CrostonMSEAUTO, CrostonMASEAUTO, CrostonRMSSEAUTO))

####          Croston on OIL data ####
CrostonOIL <- data.frc(trainOIL, method="crost", h=17, w=NULL,nop=2, type="croston",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

# Forecasting accuracy measures.
CrostonMSEOIL <- NULL
CrostonMASEOIL <- NULL
CrostonRMSSEOIL <- NULL
for (i in 1:ncol(testOIL)){
  CrostonMSEOIL <- cbind(CrostonMSEOIL,  MSE(t(testOIL[i]),t(CrostonOIL[i])))
  CrostonMASEOIL <- cbind(CrostonMASEOIL,  MASE(t(testOIL[i]),t(CrostonOIL[i]), mean(abs(t(trainOIL[i])))))
  CrostonRMSSEOIL <- cbind(CrostonRMSSEOIL,  RMSSE(t(testOIL[i]),t(CrostonOIL[i]), mean(abs(t(trainOIL[i])))))
}
CrostonMSEOIL <- mean(CrostonMSEOIL)
CrostonMASEOIL <- mean(CrostonMASEOIL)
CrostonRMSSEOIL <- mean(CrostonRMSSEOIL)
print(c("AUTO", CrostonMSEOIL, CrostonMASEOIL, CrostonRMSSEOIL))

print("time Croston total")
proc.time() - ptm


####          Croston tradeoff curves ####

# Creating the tradeoff curves.
ggplot() + 
  geom_line(data=ServicelevelCrostonSIM4, aes(x=achievedfillrateCrostonSIM4, y=CrostonholdingSIM4), color='green') + 
  geom_point(data=ServicelevelCrostonSIM4, aes(x=achievedfillrateCrostonSIM4, y=CrostonholdingSIM4), size = 1)+ 
  geom_line(data=ServicelevelSESSIM4, aes(x=achievedfillrateSESSIM4, y=SESholdingSIM4), color='steelblue')+ 
  geom_point(data=ServicelevelSESSIM4, aes(x=achievedfillrateSESSIM4, y=SESholdingSIM4), size = 1)+ 
  ggtitle("Achieved fill rate vs. Inventory holding costs" ) 

ggplot() + 
  geom_line(data=ServicelevelCrostonSIM4, aes(x=targetfillrates, y=achievedfillrateCrostonSIM4), color='green') + 
  geom_point(data=ServicelevelCrostonSIM4, aes(x=targetfillrates, y=achievedfillrateCrostonSIM4), size = 1)+ 
  geom_line(data=ServicelevelSESSIM4, aes(x=targetfillrates, y=achievedfillrateSESSIM4), color='steelblue')+ 
  geom_point(data=ServicelevelSESSIM4, aes(x=targetfillrates, y=achievedfillrateSESSIM4), size = 1)+ 
  ggtitle("Target fill rate vs. Achieved fill rate")

plot1
plot2

####          Simple Exponential Smoothing on all data sets. ####
ptm <- proc.time()

####          SES on SIM1 data ####
sesSIM1 <- data.frc(trainSIM1, method="sexsm",h=18, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on SIM2 data ####
sesSIM2 <- data.frc(trainSIM2, method="sexsm", h=18, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on SIM3 data ####
sesSIM3 <- data.frc(trainSIM3, method="sexsm", h=18, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on SIM4 data ####
sesSIM4 <- data.frc(trainSIM4, method="sexsm", h=18, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out
MSE(t(testSIM4[3]),t(sesSIM4))
MASE(t(testSIM4[3]),t(sesSIM4), mean(abs(t(trainSIM4[3]))))
RMSSE(t(testSIM4[3]),t(sesSIM4), mean(abs(t(trainSIM4[3]))))

# Inventory performance measures.
targetfillrates <- c(qnorm(0.75),qnorm(0.8),qnorm(0.85),qnorm(0.9),qnorm(0.95),qnorm(0.99),qnorm(0.999999))
leadtimes <- as.data.frame(leadtimesSIM4)
prices <- as.data.frame(pricesSIM4)

i=3

holdingcosts <- matrix(c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999),ncol = 7)
for (i in 1:ncol(sesSIM4)){
  leadtimedemand <- mean(t(sesSIM4[i])) * 1 # * leadtimes[,i]
  stdev <- std(t(trainSIM4[i]))
  stocklevelR <- (targetfillrates * stdev + leadtimedemand)
  holdingcosts <- rbind(holdingcosts,(0.25 * (stocklevelR * prices[,i])))
}
SESholdingSIM4 <- colSums(holdingcosts[-1,])
ProcentualSESholdingSIM4 <- SESholdingSIM4/SESholdingSIM4[1]

SESholdingSIM4 <- holdingcosts[-1,]

# Achieved fill rate.
fillrateSESSIM4 <- matrix(ncol=7)
averagefillrateitem <- matrix(ncol=7)
fillratefiller <- matrix(rep(NA,18),ncol=nrow(testSIM4))
for(i in 1:ncol(testSIM4)){
  for (j in 1:length(targetfillrates)){
    fillratefiller <- stocklevelR[j] / t(testSIM4[i])[(t(testSIM4[i]) > 0)]
    fillratefiller[fillratefiller >= 1] <- 1
    averagefillrateitem[j] <- mean(fillratefiller)
  }
  fillrateSESSIM4 <- rbind(fillrateSESSIM4, averagefillrateitem)
}
achievedfillrateSESSIM4 <- colMeans(fillrateSESSIM4[-1,])

achievedfillrateSESSIM4 <- fillrateSESSIM4[-1,]

ServicelevelSESSIM4 <- data.frame(achievedfillrateSESSIM4 = achievedfillrateSESSIM4, SESholdingSIM4 = ProcentualSESholdingSIM4, targetfillrates = c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999))




####          SES on MAN data ####
sesMAN <- data.frc(trainMAN, method="sexsm", h=45, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on BRAF data ####
sesBRAF <- data.frc(trainBRAF, method="sexsm", h=25, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on AUTO data ####
sesAUTO <- data.frc(trainAUTO, method="sexsm", h=7, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SES on OIL data ####
sesOIL <- data.frc(trainOIL, method="sexsm", h=17, w=NULL,cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

print("time SES total")
proc.time() - ptm


####          SBA on all data sets. ####
ptm <- proc.time()

####          SBA on SIM1 data. ####
sbaSIM1 <- data.frc(trainSIM1, method="crost", h=18, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on SIM2 data. ####
sbaSIM2 <- data.frc(trainSIM2, method="crost", h=18, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on SIM3 data. ####
sbaSIM3 <- data.frc(trainSIM3, method="crost", h=18, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on SIM4 data. ####
sbaSIM4 <- data.frc(trainSIM4, method="crost", h=18, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out
MSE(t(testSIM4[3]),t(sbaSIM4))
MASE(t(testSIM4[3]),t(sbaSIM4), mean(abs(t(trainSIM4[3]))))
RMSSE(t(testSIM4[3]),t(sbaSIM4), mean(abs(t(trainSIM4[3]))))


# Inventory performance measures.
targetfillrates <- c(qnorm(0.75),qnorm(0.8),qnorm(0.85),qnorm(0.9),qnorm(0.95),qnorm(0.99),qnorm(0.999999))
leadtimes <- as.data.frame(leadtimesSIM4)
prices <- as.data.frame(pricesSIM4)

i=3

holdingcosts <- matrix(c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999),ncol = 7)
for (i in 1:ncol(sbaSIM4)){
  leadtimedemand <- mean(t(sbaSIM4[i])) * 1 # * leadtimes[,i]
  stdev <- std(t(trainSIM4[i]))
  stocklevelR <- (targetfillrates * stdev + leadtimedemand)
  holdingcosts <- rbind(holdingcosts,(0.25 * (stocklevelR * prices[,i])))
}
SBAholdingSIM4 <- colSums(holdingcosts[-1,])
ProcentualSBAholdingSIM4 <- SBAholdingSIM4/SBAholdingSIM4[1]

SBAholdingSIM4 <- holdingcosts[-1,]

# Achieved fill rate.
fillrateSBASIM4 <- matrix(ncol=7)
averagefillrateitem <- matrix(ncol=7)
fillratefiller <- matrix(rep(NA,18),ncol=nrow(testSIM4))
for(i in 1:ncol(testSIM4)){
  for (j in 1:length(targetfillrates)){
    fillratefiller <- stocklevelR[j] / t(testSIM4[i])[(t(testSIM4[i]) > 0)]
    fillratefiller[fillratefiller >= 1] <- 1
    averagefillrateitem[j] <- mean(fillratefiller)
  }
  fillrateSBASIM4 <- rbind(fillrateSBASIM4, averagefillrateitem)
}
achievedfillrateSBASIM4 <- colMeans(fillrateSBASIM4[-1,])

achievedfillrateSBASIM4 <- fillrateSBASIM4[-1,]

ServicelevelSBASIM4 <- data.frame(achievedfillrateSBASIM4 = achievedfillrateSBASIM4, SBAholdingSIM4 = ProcentualSBAholdingSIM4, targetfillrates = c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999))


####          SBA on MAN data. ####
sbaMAN <- data.frc(trainMAN, method="crost", h=45, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on BRAF data. ####
sbaBRAF <- data.frc(trainBRAF, method="crost", h=25, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on AUTO data. ####
sbaAUTO <- data.frc(trainAUTO, method="crost", h=7, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          SBA on OIL data. ####
sbaOIL <- data.frc(trainOIL, method="crost", h=17, w=NULL, type="sba",cost="mar",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

print("time SBA total")
proc.time() - ptm

####          TSB on all data sets. ####
ptm <- proc.time()

####          TSB on SIM1 data. ####
tsbSIM1 <- data.frc(trainSIM1, method="tsb", h=18, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          TSB on SIM2 data. ####
tsbSIM2 <- data.frc(trainSIM2, method="tsb", h=18, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          TSB on SIM3 data. ####
tsbSIM3 <- data.frc(trainSIM3, method="tsb", h=18, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

####          TSB on SIM4 data. ####
tsbSIM4 <- data.frc(trainSIM4, method="tsb", h=18, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out



# Inventory performance measures.
targetfillrates <- c(qnorm(0.75),qnorm(0.8),qnorm(0.85),qnorm(0.9),qnorm(0.95),qnorm(0.99),qnorm(0.999999))
leadtimes <- as.data.frame(leadtimesSIM4)
prices <- as.data.frame(pricesSIM4)

i=3

holdingcosts <- matrix(c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999),ncol = 7)
for (i in 1:ncol(sbaSIM4)){
  leadtimedemand <- mean(t(sbaSIM4[i])) * 1 # * leadtimes[,i]
  stdev <- std(t(trainSIM4[i]))
  stocklevelR <- (targetfillrates * stdev + leadtimedemand)
  holdingcosts <- rbind(holdingcosts,(0.25 * (stocklevelR * prices[,i])))
}
SBAholdingSIM4 <- colSums(holdingcosts[-1,])
ProcentualSBAholdingSIM4 <- SBAholdingSIM4/SBAholdingSIM4[1]

SBAholdingSIM4 <- holdingcosts[-1,]

# Achieved fill rate.
fillrateSBASIM4 <- matrix(ncol=7)
averagefillrateitem <- matrix(ncol=7)
fillratefiller <- matrix(rep(NA,18),ncol=nrow(testSIM4))
for(i in 1:ncol(testSIM4)){
  for (j in 1:length(targetfillrates)){
    fillratefiller <- stocklevelR[j] / t(testSIM4[i])[(t(testSIM4[i]) > 0)]
    fillratefiller[fillratefiller >= 1] <- 1
    averagefillrateitem[j] <- mean(fillratefiller)
  }
  fillrateSBASIM4 <- rbind(fillrateSBASIM4, averagefillrateitem)
}
achievedfillrateSBASIM4 <- colMeans(fillrateSBASIM4[-1,])

achievedfillrateSBASIM4 <- fillrateSBASIM4[-1,]

ServicelevelSBASIM4 <- data.frame(achievedfillrateSBASIM4 = achievedfillrateSBASIM4, SBAholdingSIM4 = ProcentualSBAholdingSIM4, targetfillrates = c(0.75,0.8,0.85,0.9,0.95,0.99,0.9999999))




####          TSB on MAN data. ####
tsbMAN <- data.frc(trainMAN, method="tsb", h=45, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out


####          TSB on BRAF data. ####
tsbBRAF <- data.frc(trainBRAF, method="tsb", h=25, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out


####          TSB on AUTO data. ####
tsbAUTO <- data.frc(trainAUTO, method="tsb", h=7, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out


####          TSB on OIL data. ####
tsbOIL <- data.frc(trainOIL, method="tsb", h=17, w=NULL,cost="msr",init="naive", init.opt=TRUE, na.rm=TRUE)$frc.out

print("time TSB total")
proc.time() - ptm

####          Willemain bootstrapping on all data sets. ####

# Create a running index and timer.
i=1
y=1
ptm <- proc.time()

# Defining state names for the Markov Chain.
# Defining the data sets as sequences and replacing the nonzeroes with 1's.
seqSIM1 <- seqdef(trainSIM1)
seqSIM1[trainSIM1 != 0] <- 1
seqSIM1 <- seqdef(seqSIM1)

seqSIM2 <- seqdef(trainSIM2)
seqSIM2[trainSIM2 != 0] <- 1
seqSIM2 <- seqdef(seqSIM2)

seqSIM3 <- seqdef(trainSIM3)
seqSIM3[trainSIM3 != 0] <- 1
seqSIM3 <- seqdef(seqSIM3)

seqSIM4 <- seqdef(trainSIM4)
seqSIM4[trainSIM4 != 0] <- 1
seqSIM4 <- seqdef(seqSIM4)

seqMAN <- seqdef(trainMAN)
seqMAN[trainMAN != 0] <- 1
seqMAN <- seqdef(seqMAN)

seqBRAF <- seqdef(trainBRAF)
seqBRAF[trainBRAF != 0] <- 1
seqBRAF <- seqdef(seqBRAF)

seqAUTO <- seqdef(trainAUTO)
seqAUTO[trainAUTO != 0] <- 1
seqAUTO <- seqdef(seqAUTO)

seqOIL <- seqdef(trainOIL)
seqOIL[trainOIL != 0] <- 1
seqOIL[seqOIL != 1] <- 0
seqOIL <- seqdef(seqOIL)

# Establishing output variables for each data set. 
ltdSIM1=0
WillemainSIM1=0
WillemainSDSIM1=0

ltdSIM2=0
WillemainSIM2=0
WillemainSDSIM2=0

ltdSIM3=0
WillemainSIM3=0
WillemainSDSIM3=0

ltdSIM4=0
WillemainSIM4=0
WillemainSDSIM4=0

ltdMAN=0
WillemainMAN=0
WillemainSDMAN=0

ltdBRAF=0
WillemainBRAF=0
WillemainSDBRAF=0

ltdAUTO=0
WillemainAUTO=0
WillemainSDAUTO=0

ltdOIL=0
WillemainOIL=0
WillemainSDOIL=0

# Willemain outputs for each data set.
ptm <- proc.time()

# SIM1
statesNames=c(1) # SIM1 has no zeroes, so a 1x1 matrix is used here instead of the 2x2 for the other transition probability matrices.
i=1
y=1
for(i in 1:ncol(seqSIM1)){
  leadtime <- 1
  transprob <- createSequenceMatrix(
    seqSIM1[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                 nrow=1, dimnames=list(statesNames,statesNames)))
  for (y in 1:10){
    mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM1[[i]], n=1),include.t0 = FALSE))
    nonzeroamount <- length(mcprediction[mcprediction != 0])
    nonzeroes <- trainSIM1[[i]]
    mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
    mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
    ltdSIM1[[y]] <- sum(mcprediction)
  }
  WillemainSIM1[[i]] <- mean(ltdSIM1)
  WillemainSDSIM1[[i]] <- sd(ltdSIM1)
}

# SIM2
statesNames=c(0,1)
i=1
y=1
for(i in 1:ncol(seqSIM2)){
  leadtime <- 1
  transprob <- createSequenceMatrix(
    seqSIM2[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                 nrow=2, dimnames=list(statesNames,statesNames)))
  for (y in 1:10){
    mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM2[[i]], n=1),include.t0 = FALSE))
    nonzeroamount <- length(mcprediction[mcprediction != 0])
    nonzeroes <- trainSIM2[[i]]
    mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
    mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
    ltdSIM2[[y]] <- sum(mcprediction)
  }
  WillemainSIM2[[i]] <- mean(ltdSIM2)
  WillemainSDSIM2[[i]] <- sd(ltdSIM2)
}

# SIM3
# Items with only nonzeroes and one zero at the end, breaking the transition probability matrix. 
# These items were replaced with another item from the data set. 
seqSIM3[[158]] <- seqSIM3[[157]]
seqSIM3[[524]] <- seqSIM3[[157]]
seqSIM3[[666]] <- seqSIM3[[157]]
seqSIM3[[692]] <- seqSIM3[[157]]
seqSIM3[[751]] <- seqSIM3[[157]]
seqSIM3[[795]] <- seqSIM3[[157]]
seqSIM3[[843]] <- seqSIM3[[157]]
seqSIM3[[1252]] <- seqSIM3[[157]]
seqSIM3[[1503]] <- seqSIM3[[157]]
seqSIM3[[1555]] <- seqSIM3[[157]]
seqSIM3[[1650]] <- seqSIM3[[157]]
seqSIM3[[1893]] <- seqSIM3[[157]]
seqSIM3[[2069]] <- seqSIM3[[157]]
seqSIM3[[2381]] <- seqSIM3[[157]]
seqSIM3[[2414]] <- seqSIM3[[157]]
seqSIM3[[2451]] <- seqSIM3[[157]]
seqSIM3[[2623]] <- seqSIM3[[157]]
seqSIM3[[2700]] <- seqSIM3[[157]]
seqSIM3[[2809]] <- seqSIM3[[157]]
seqSIM3[[3011]] <- seqSIM3[[157]]
seqSIM3[[3406]] <- seqSIM3[[157]]
seqSIM3[[3520]] <- seqSIM3[[157]]
seqSIM3[[3668]] <- seqSIM3[[157]]
seqSIM3[[3670]] <- seqSIM3[[157]]
seqSIM3[[3701]] <- seqSIM3[[157]]
seqSIM3[[4155]] <- seqSIM3[[157]]
seqSIM3[[4807]] <- seqSIM3[[157]]
seqSIM3[[4841]] <- seqSIM3[[157]]
seqSIM3[[5143]] <- seqSIM3[[157]]
seqSIM3[[5269]] <- seqSIM3[[157]]
seqSIM3[[5841]] <- seqSIM3[[157]]
seqSIM3[[5860]] <- seqSIM3[[157]]
seqSIM3[[6004]] <- seqSIM3[[157]]
seqSIM3[[6294]] <- seqSIM3[[157]]
seqSIM3[[6371]] <- seqSIM3[[157]]
seqSIM3[[6499]] <- seqSIM3[[157]]

i=1
y=1
for(i in 1:ncol(seqSIM3)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqSIM3[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
  mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                 nrow=2, dimnames=list(statesNames,statesNames)))
  for (y in 1:10){
    mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM3[[i]], n=1),include.t0 = FALSE))
    nonzeroamount <- length(mcprediction[mcprediction != 0])
    nonzeroes <- trainSIM3[[i]]
    mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
    mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
    ltdSIM3[[y]] <- sum(mcprediction)
  }
  WillemainSIM3[[i]] <- mean(ltdSIM3)
  WillemainSDSIM3[[i]] <- sd(ltdSIM3)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM3[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainSIM3[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdSIM3[[y]] <- sum(mcprediction)
  }
  WillemainSIM3[[i]] <- mean(ltdSIM3)
  WillemainSDSIM3[[i]] <- sd(ltdSIM3)
}}

# SIM4
i=1
y=1
for(i in 1:ncol(seqSIM4)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqSIM4[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
    mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                   nrow=2, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM4[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainSIM4[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdSIM4[[y]] <- sum(mcprediction)
    }
    WillemainSIM4[[i]] <- mean(ltdSIM4)
    WillemainSDSIM4[[i]] <- sd(ltdSIM4)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqSIM4[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainSIM4[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdSIM4[[y]] <- sum(mcprediction)
    }
    WillemainSIM4[[i]] <- mean(ltdSIM4)
    WillemainSDSIM4[[i]] <- sd(ltdSIM4)
    }}

# MAN
i=1
y=1
for(i in 1:ncol(seqMAN)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqMAN[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
    mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                   nrow=2, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqMAN[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainMAN[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdMAN[[y]] <- sum(mcprediction)
    }
    WillemainMAN[[i]] <- mean(ltdMAN)
    WillemainSDMAN[[i]] <- sd(ltdMAN)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqMAN[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainMAN[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdMAN[[y]] <- sum(mcprediction)
    }
    WillemainMAN[[i]] <- mean(ltdMAN)
    WillemainSDMAN[[i]] <- sd(ltdMAN)
    }}

# BRAF
i=1
y=1
for(i in 1:ncol(seqBRAF)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqBRAF[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
    mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                   nrow=2, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqBRAF[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainBRAF[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdBRAF[[y]] <- sum(mcprediction)
    }
    WillemainBRAF[[i]] <- mean(ltdBRAF)
    WillemainSDBRAF[[i]] <- sd(ltdBRAF)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqBRAF[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainBRAF[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdBRAF[[y]] <- sum(mcprediction)
    }
    WillemainBRAF[[i]] <- mean(ltdBRAF)
    WillemainSDBRAF[[i]] <- sd(ltdBRAF)
    }}

# AUTO
# Items with only nonzeroes and one zero at the end, breaking the transition probability matrix. 
# These items were replaced with another item from the data set. 
seqAUTO[[17]] <- seqAUTO[[2]]
seqAUTO[[110]] <- seqAUTO[[2]]
seqAUTO[[256]] <- seqAUTO[[2]]
seqAUTO[[298]] <- seqAUTO[[2]]
seqAUTO[[363]] <- seqAUTO[[2]]
seqAUTO[[393]] <- seqAUTO[[2]]
seqAUTO[[471]] <- seqAUTO[[2]]
seqAUTO[[602]] <- seqAUTO[[2]]
seqAUTO[[659]] <- seqAUTO[[2]]
seqAUTO[[783]] <- seqAUTO[[2]]
seqAUTO[[857]] <- seqAUTO[[2]]
seqAUTO[[860]] <- seqAUTO[[2]]
seqAUTO[[908]] <- seqAUTO[[2]]
seqAUTO[[930]] <- seqAUTO[[2]]
seqAUTO[[1007]] <- seqAUTO[[2]]
seqAUTO[[1158]] <- seqAUTO[[2]]
seqAUTO[[1163]] <- seqAUTO[[2]]
seqAUTO[[1221]] <- seqAUTO[[2]]
seqAUTO[[1321]] <- seqAUTO[[2]]
seqAUTO[[1410]] <- seqAUTO[[2]]
seqAUTO[[1693]] <- seqAUTO[[2]]
seqAUTO[[1736]] <- seqAUTO[[2]]
seqAUTO[[1826]] <- seqAUTO[[2]]
seqAUTO[[1935]] <- seqAUTO[[2]]
seqAUTO[[1981]] <- seqAUTO[[2]]
seqAUTO[[2028]] <- seqAUTO[[2]]
seqAUTO[[2059]] <- seqAUTO[[2]]
seqAUTO[[2067]] <- seqAUTO[[2]]
seqAUTO[[2152]] <- seqAUTO[[2]]
seqAUTO[[2402]] <- seqAUTO[[2]]
seqAUTO[[2551]] <- seqAUTO[[2]]
seqAUTO[[2563]] <- seqAUTO[[2]]
seqAUTO[[2671]] <- seqAUTO[[2]]
seqAUTO[[2691]] <- seqAUTO[[2]]
seqAUTO[[2693]] <- seqAUTO[[2]]
seqAUTO[[2766]] <- seqAUTO[[2]]
seqAUTO[[2921]] <- seqAUTO[[2]]

i=1
y=1
for(i in 1:ncol(seqAUTO)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqAUTO[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
    mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                   nrow=2, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqAUTO[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainAUTO[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdAUTO[[y]] <- sum(mcprediction)
    }
    WillemainAUTO[[i]] <- mean(ltdAUTO)
    WillemainSDAUTO[[i]] <- sd(ltdAUTO)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqAUTO[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainAUTO[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdAUTO[[y]] <- sum(mcprediction)
    }
    WillemainAUTO[[i]] <- mean(ltdAUTO)
    WillemainSDAUTO[[i]] <- sd(ltdAUTO)
    }}

# OIL
i=1
y=1
for(i in 1:ncol(seqOIL)){
  leadtime <- 1
  statesNames=c(0,1)
  transprob <- createSequenceMatrix(
    seqOIL[[i]],
    toRowProbs = TRUE,
    sanitize = FALSE
  )
  if(length(transprob)!=1){
    mc<-new("markovchain", transitionMatrix=matrix((transprob),
                                                   nrow=2, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqOIL[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- na.omit(trainOIL[[i]])
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdOIL[[y]] <- sum(mcprediction)
    }
    WillemainOIL[[i]] <- mean(ltdOIL)
    WillemainSDOIL[[i]] <- sd(ltdOIL)} else {statesNames=c(1)
    mc<-new("markovchain", transitionMatrix=matrix((transprob),byrow=TRUE,
                                                   nrow=1, dimnames=list(statesNames,statesNames)))
    for (y in 1:10){
      mcprediction <- as.numeric(markovchainSequence(n=leadtime, markovchain=mc,t0 = tail(seqOIL[[i]], n=1),include.t0 = FALSE))
      nonzeroamount <- length(mcprediction[mcprediction != 0])
      nonzeroes <- trainOIL[[i]]
      mcprediction[mcprediction != 0] <- sample(nonzeroes[nonzeroes != 0], size=nonzeroamount)
      mcprediction[mcprediction != 0] <- 1 + round(mcprediction[mcprediction != 0] + rnorm(1)*sqrt(mcprediction[mcprediction != 0]))
      ltdOIL[[y]] <- sum(mcprediction)
    }
    WillemainOIL[[i]] <- mean(ltdOIL)
    WillemainSDOIL[[i]] <- sd(ltdOIL)
    }}

print("time Willemain total")
proc.time() - ptm

####          Machine learning method (Single hidden layer MLP) on all data sets. ####
# Normalizing the data to 0-1 for the MLP, ignoring NA's for the OIL dataset using a custom function.
normalize <- function(x) {
  return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))}

nndataSIM1 <- normalizeData(SIM1, type = "0_1")
nndataSIM2 <- normalizeData(SIM2, type = "0_1")
nndataSIM3 <- normalizeData(SIM3, type = "0_1")
nndataSIM4 <- normalizeData(SIM4, type = "0_1")
nndataMAN  <- normalizeData(MAN, type = "0_1")
nndataBRAF <- normalizeData(BRAF, type = "0_1")
nndataAUTO <- normalizeData(AUTO, type = "0_1")
nndataOIL <- OIL
n=1
for (n in 1:ncol(OIL)){
  nndataOIL[[n]]  <- normalize(OIL[[n]])
}

# Splitting the data again into test and train data.

sampleSIM1 = round(nrow(SIM1)*.70) 
nntrainSIM1 <- nndataSIM1[1:(sampleSIM1), ]
nntestSIM1 <- nndataSIM1[-(1:(sampleSIM1)), ]

nntrainSIM2 <- nndataSIM2[1:(sampleSIM1), ]
nntestSIM2 <- nndataSIM2[-(1:(sampleSIM1)), ]

nntrainSIM3 <- nndataSIM3[1:(sampleSIM1), ]
nntestSIM3 <- nndataSIM3[-(1:(sampleSIM1)), ]

nntrainSIM4 <- nndataSIM4[1:(sampleSIM1), ]
nntestSIM4 <- nndataSIM4[-(1:(sampleSIM1)), ]

# The split point is established for the industrial data sets individually, again.
sampleMAN = round(nrow(MAN)*.70)
nntrainMAN <- nndataMAN[1:(sampleMAN), ]
nntestMAN <- nndataMAN[-(1:(sampleMAN)), ] 

sampleBRAF = round(nrow(BRAF)*.70)
nntrainBRAF <- nndataBRAF[1:(sampleBRAF), ]
nntestBRAF <- nndataBRAF[-(1:(sampleBRAF)), ]

sampleAUTO = round(nrow(AUTO)*.70)
nntrainAUTO <- nndataAUTO[1:(sampleAUTO), ]
nntestAUTO <- nndataAUTO[-(1:(sampleAUTO)), ]

sampleOIL = round(nrow(OIL)*.70)
nntrainOIL <- nndataOIL[1:(sampleOIL), ]
nntestOIL <- nndataOIL[-(1:(sampleOIL)), ]

####          SIM1 neural network application.          ####
# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM1)){
nndatavector <- nntrainSIM1[,i]
for (t in 1:length(nndatavector)){
inputs <- nndatavector[(0+t):(4+t)]
outputs <- nndatavector[5+t]
nndata <- rbind(nndata, c(inputs,outputs))  
}}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM1)
predictionnnSIM1 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainSIM1)){
input <- tail(nntrainSIM1[,i],n=5)
outputs <- vector()
for (t in 1:nrow(testSIM1)) {
output <- predict(model, t(input))
outputs <- c(outputs, output)
input <- c(input,output)
input <- tail(input, n=5)
}
predictionnnSIM1 <- rbind(predictionnnSIM1, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM1 <- getNormParameters(nndataSIM1)
predictionnnSIM1 <- t(predictionnnSIM1)
predictionnnSIM1 <- predictionnnSIM1[,-1]
# Final predictions.
predictionnnSIM1 <- denormalizeData(predictionnnSIM1, normParamsSIM1)
nntestSIM1 <- denormalizeData(nntestSIM1, normParamsSIM1)

####          SIM2 neural network application.          ####
# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM2)){
  nndatavector <- nntrainSIM2[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM2)
predictionnnSIM2 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainSIM2)){
  input <- tail(nntrainSIM2[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM2)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnSIM2 <- rbind(predictionnnSIM2, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM2 <- getNormParameters(nndataSIM2)
predictionnnSIM2 <- t(predictionnnSIM2)
predictionnnSIM2 <- predictionnnSIM2[,-1]
# Final predictions.
predictionnnSIM2 <- denormalizeData(predictionnnSIM2, normParamsSIM2)
nntestSIM2 <- denormalizeData(nntestSIM2, normParamsSIM2)


####          SIM3 neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM3)){
  nndatavector <- nntrainSIM3[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM3)
predictionnnSIM3 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainSIM3)){
  input <- tail(nntrainSIM3[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM3)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnSIM3 <- rbind(predictionnnSIM3, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM3 <- getNormParameters(nndataSIM3)
predictionnnSIM3 <- t(predictionnnSIM3)
predictionnnSIM3 <- predictionnnSIM3[,-1]
# Final predictions.
predictionnnSIM3 <- denormalizeData(predictionnnSIM3, normParamsSIM3)
nntestSIM3 <- denormalizeData(nntestSIM3, normParamsSIM3)


####          SIM4 neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM4)){
  nndatavector <- nntrainSIM4[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM4)
predictionnnSIM4 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainSIM4)){
  input <- tail(nntrainSIM4[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM4)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnSIM4 <- rbind(predictionnnSIM4, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM4 <- getNormParameters(nndataSIM4)
predictionnnSIM4 <- t(predictionnnSIM4)
predictionnnSIM4 <- predictionnnSIM4[,-1]
# Final predictions.
predictionnnSIM4 <- denormalizeData(predictionnnSIM4, normParamsSIM4)
nntestSIM4 <- denormalizeData(nntestSIM4, normParamsSIM4)


####          MAN neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainMAN)){
  nndatavector <- nntrainMAN[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testMAN)
predictionnnMAN <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainMAN)){
  input <- tail(nntrainMAN[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testMAN)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnMAN <- rbind(predictionnnMAN, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsMAN <- getNormParameters(nndataMAN)
predictionnnMAN <- t(predictionnnMAN)
predictionnnMAN <- predictionnnMAN[,-1]
# Final predictions.
predictionnnMAN <- denormalizeData(predictionnnMAN, normParamsMAN)
nntestMAN <- denormalizeData(nntestMAN, normParamsMAN)

####          BRAF neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainBRAF)){
  nndatavector <- nntrainBRAF[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testBRAF)
predictionnnBRAF <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainBRAF)){
  input <- tail(nntrainBRAF[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testBRAF)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnBRAF <- rbind(predictionnnBRAF, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsBRAF <- getNormParameters(nndataBRAF)
predictionnnBRAF <- t(predictionnnBRAF)
predictionnnBRAF <- predictionnnBRAF[,-1]
# Final predictions.
predictionnnBRAF <- denormalizeData(predictionnnBRAF, normParamsBRAF)
nntestBRAF <- denormalizeData(nntestBRAF, normParamsBRAF)

####          AUTO neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainAUTO)){
  nndatavector <- nntrainAUTO[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testAUTO)
predictionnnAUTO <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainAUTO)){
  input <- tail(nntrainAUTO[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testAUTO)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnAUTO <- rbind(predictionnnAUTO, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsAUTO <- getNormParameters(nndataAUTO)
predictionnnAUTO <- t(predictionnnAUTO)
predictionnnAUTO <- predictionnnAUTO[,-1]
# Final predictions.
predictionnnAUTO <- denormalizeData(predictionnnAUTO, normParamsAUTO)
nntestAUTO <- denormalizeData(nntestAUTO, normParamsAUTO)

####          OIL neural network application.          ####

# Creating running indices and new variables.
nndata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainOIL)){
  nndatavector <- nntrainOIL[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    nndata <- rbind(nndata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
nndata <- na.omit(nndata)

# Training the Neural Network.
model <- mlp(nndata[,1:5], nndata[,6], size=3, learnFuncParams=c(0.1),
             maxit=50)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testOIL)
predictionnnOIL <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictionsnndata.
for (i in 1:ncol(nntrainOIL)){
  input <- tail(nntrainOIL[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testOIL)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionnnOIL <- rbind(predictionnnOIL, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsOIL <- getNormParameters(nndataOIL)
predictionnnOIL <- t(predictionnnOIL)
predictionnnOIL <- predictionnnOIL[,-1]
# Final predictions.
predictionnnOIL <- denormalizeData(predictionnnOIL, normParamsOIL)
nntestOIL <- denormalizeData(nntestOIL, normParamsOIL)


####          SIM1 LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM1)){
  nndatavector <- nntrainSIM1[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM1)
predictionLightGBMSIM1 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainSIM1)){
  input <- tail(nntrainSIM1[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM1)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMSIM1 <- rbind(predictionLightGBMSIM1, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM1 <- getNormParameters(nndataSIM1)
predictionLightGBMSIM1 <- t(predictionLightGBMSIM1)
predictionLightGBMSIM1 <- predictionLightGBMSIM1[,-1]
# Final predictions.
predictionLightGBMSIM1 <- denormalizeData(predictionLightGBMSIM1, normParamsSIM1)
lightgbmtestSIM1 <- denormalizeData(nntestSIM1, normParamsSIM1)


####          SIM2 LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM2)){
  nndatavector <- nntrainSIM2[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM2)
predictionLightGBMSIM2 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainSIM2)){
  input <- tail(nntrainSIM2[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM2)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMSIM2 <- rbind(predictionLightGBMSIM2, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM2 <- getNormParameters(nndataSIM2)
predictionLightGBMSIM2 <- t(predictionLightGBMSIM2)
predictionLightGBMSIM2 <- predictionLightGBMSIM2[,-1]
# Final predictions.
predictionLightGBMSIM2 <- denormalizeData(predictionLightGBMSIM2, normParamsSIM2)
lightgbmtestSIM2 <- denormalizeData(nntestSIM2, normParamsSIM2)



####          SIM3 LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM3)){
  nndatavector <- nntrainSIM3[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM3)
predictionLightGBMSIM3 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainSIM3)){
  input <- tail(nntrainSIM3[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM3)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMSIM3 <- rbind(predictionLightGBMSIM3, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM3 <- getNormParameters(nndataSIM3)
predictionLightGBMSIM3 <- t(predictionLightGBMSIM3)
predictionLightGBMSIM3 <- predictionLightGBMSIM3[,-1]
# Final predictions.
predictionLightGBMSIM3 <- denormalizeData(predictionLightGBMSIM3, normParamsSIM3)
lightgbmtestSIM3 <- denormalizeData(nntestSIM3, normParamsSIM3)



####          SIM4 LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainSIM4)){
  nndatavector <- nntrainSIM4[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM4)
predictionLightGBMSIM4 <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainSIM4)){
  input <- tail(nntrainSIM4[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testSIM4)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMSIM4 <- rbind(predictionLightGBMSIM4, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsSIM4 <- getNormParameters(nndataSIM4)
predictionLightGBMSIM4 <- t(predictionLightGBMSIM4)
predictionLightGBMSIM4 <- predictionLightGBMSIM4[,-1]
# Final predictions.
predictionLightGBMSIM4 <- denormalizeData(predictionLightGBMSIM4, normParamsSIM4)
lightgbmtestSIM4 <- denormalizeData(nntestSIM4, normParamsSIM4)



####          MAN LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainMAN)){
  nndatavector <- nntrainMAN[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testMAN)
predictionLightGBMMAN <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainMAN)){
  input <- tail(nntrainMAN[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testMAN)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMMAN <- rbind(predictionLightGBMMAN, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsMAN <- getNormParameters(nndataMAN)
predictionLightGBMMAN <- t(predictionLightGBMMAN)
predictionLightGBMMAN <- predictionLightGBMMAN[,-1]
# Final predictions.
predictionLightGBMMAN <- denormalizeData(predictionLightGBMMAN, normParamsMAN)
lightgbmtestMAN <- denormalizeData(nntestMAN, normParamsMAN)



####          BRAF LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainBRAF)){
  nndatavector <- nntrainBRAF[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testBRAF)
predictionLightGBMBRAF <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainBRAF)){
  input <- tail(nntrainBRAF[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testBRAF)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMBRAF <- rbind(predictionLightGBMBRAF, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsBRAF <- getNormParameters(nndataBRAF)
predictionLightGBMBRAF <- t(predictionLightGBMBRAF)
predictionLightGBMBRAF <- predictionLightGBMBRAF[,-1]
# Final predictions.
predictionLightGBMBRAF <- denormalizeData(predictionLightGBMBRAF, normParamsBRAF)
lightgbmtestBRAF <- denormalizeData(nntestBRAF, normParamsBRAF)



####          AUTO LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainAUTO)){
  nndatavector <- nntrainAUTO[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testAUTO)
predictionLightGBMAUTO <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainAUTO)){
  input <- tail(nntrainAUTO[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testAUTO)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMAUTO <- rbind(predictionLightGBMAUTO, outputs)
}

# Denormalizing the data for evaluation of forecasts.
normParamsAUTO <- getNormParameters(nndataAUTO)
predictionLightGBMAUTO <- t(predictionLightGBMAUTO)
predictionLightGBMAUTO <- predictionLightGBMAUTO[,-1]
# Final predictions.
predictionLightGBMAUTO <- denormalizeData(predictionLightGBMAUTO, normParamsAUTO)
lightgbmtestAUTO <- denormalizeData(nntestAUTO, normParamsAUTO)



####          OIL LightGBM method application.         ####

# Creating running indices and new variables.
lightgbmdata <- matrix(ncol = 6)
i <- 1
t <- 1

# For loop that creates a 6 column data set with the first 5 as inputs to the neural net and the 6th column as the target variable.
for (i in 1:ncol(nntrainOIL)){
  nndatavector <- nntrainOIL[,i]
  for (t in 1:length(nndatavector)){
    inputs <- nndatavector[(0+t):(4+t)]
    outputs <- nndatavector[5+t]
    lightgbmdata <- rbind(lightgbmdata, c(inputs,outputs))  
  }}

# Removing the rows with NA's. Since the data is large enough already to learn the characteristics of the data, this should not decrease accuracy.
lightgbmdata <- na.omit(lightgbmdata)
lightgbmdata <- as.data.frame(lightgbmdata)

# Setting the hyperparameters for the LightGBM model, based on the kaggle method.
p <- list(objective = "regression",
          metric ="rmse",
          boosting = "gbdt",
          force_row_wise = TRUE,
          learning_rate = 1.0,
          num_leaves = 128,
          min_data = 100,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          max_depth = 10,
          nthread = 4)

# Training the LightGBM algorithm.
model <- lightgbm(
  data = as.matrix(lightgbmdata[,(1:5)])
  , params = p
  , label = lightgbmdata$V6
  , nrounds = 4000,
  early_stopping_rounds = 400,
  eval_freq = 400)

# Resetting the running indices to be sure.
i <- 1
t <- 1
h <- nrow(testSIM1)
predictionLightGBMOIL <- matrix(ncol = h)

# Predicting the forecast horizon with as input the last five values of a series and saving it as the predictiondata.
for (i in 1:ncol(nntrainOIL)){
  input <- tail(nntrainOIL[,i],n=5)
  outputs <- vector()
  for (t in 1:nrow(testOIL)) {
    output <- predict(model, t(input))
    outputs <- c(outputs, output)
    input <- c(input,output)
    input <- tail(input, n=5)
  }
  predictionLightGBMOIL <- rbind(predictionLightGBMOIL, outputs)
}


# Saving the predictions without transposed and without column 1.
predictionLightGBMOIL <- t(predictionLightGBMOIL)
predictionLightGBMOIL <- predictionLightGBMOIL[,-1]

# Denormalizing the data for evaluation of forecasts.
i=1
for (i in 1:ncol(predictionLightGBMOIL)) {
predictionLightGBMOIL[,i] <- predictionLightGBMOIL[,i] * (max(OIL[i], na.rm=TRUE) - min(OIL[i], na.rm=TRUE)) + min(OIL[i], na.rm=TRUE)
}

i=1
for (i in 1:ncol(nntestOIL)) {
  nntestOIL[,i] <- nntestOIL[,i] * (max(OIL[i], na.rm=TRUE) - min(OIL[i], na.rm=TRUE)) + min(OIL[i], na.rm=TRUE)
}








